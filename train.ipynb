{
 "metadata": {
  "name": "",
  "signature": "sha256:58a10a0afcb01e438bc2cfd6cb91dba83caa9a9a8579fddeadd873efc5434708"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ale_python_interface import ALEInterface\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import cv2\n",
      "import random\n",
      "import threading\n",
      "\n",
      "import sys\n",
      "import time\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from replayMemory import ReplayMemory\n",
      "from buildGraph import createQNetwork, build_train_op"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/cgel/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
        "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale = ALEInterface()\n",
      "viz = False\n",
      "rom_name = \"roms/Breakout.bin\"\n",
      "ale.setBool('sound', False)                                                                                \n",
      "ale.setBool('display_screen', viz) \n",
      "ale.setInt(\"frame_skip\", 4)\n",
      "ale.loadROM(rom_name)\n",
      "legal_actions = ale.getMinimalActionSet()\n",
      "action_map = {}\n",
      "for i in range(len(legal_actions)):\n",
      "    action_map[i] = legal_actions[i]\n",
      "action_num = len(action_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class config:\n",
      "    batch_size = 32\n",
      "    action_num = action_num\n",
      "    replay_memory_capacity = 1000000\n",
      "    steps_before_training = 50000\n",
      "    buff_size = 4\n",
      "    device = \"/gpu:0\"\n",
      "    alpha = 0.9\n",
      "    gamma = 0.99\n",
      "    learning_rate = 0.00025\n",
      "    h_to_h = \"oh_concat\"\n",
      "    #h_to_h = \"expanded_concat\"\n",
      "    #h_to_h = \"conditional\"\n",
      "    exploration_steps = 1000000\n",
      "    initial_epsilon = 1.\n",
      "    final_epsilon = 0.1\n",
      "    sync_rate = 10000\n",
      "    save_summary_rate = 5000   \n",
      "    \n",
      "def get_epsilon():\n",
      "    if global_step < config.exploration_steps:\n",
      "        return config.initial_epsilon-((config.initial_epsilon-config.final_epsilon)/config.exploration_steps)*global_step\n",
      "    else:\n",
      "        return config.final_epsilon\n",
      "    \n",
      "RM = ReplayMemory(config)\n",
      "\n",
      "def flush_print(str):\n",
      "    print(str)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "def preprocess(new_frame, state):\n",
      "    frame = cv2.resize(new_frame, (84, 110))[26:110,:]\n",
      "    new_state = np.roll(state, -1, axis=3)\n",
      "    new_state[0, :, :, config.buff_size -1] = frame\n",
      "    return new_state"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.device(config.device):\n",
      "    input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"input_state_ph\")\n",
      "    # this should be: input_state_placeholder = tf.placeholder(\"float\",[None,84,84,4], name=\"state_placeholder\")\n",
      "    action_ph = tf.placeholder(tf.int64, [config.batch_size], name=\"Action_ph\")\n",
      "    Y_ph = tf.placeholder(tf.float32, [config.batch_size], name=\"Y_ph\")\n",
      "    next_Y_ph = tf.placeholder(tf.float32, [config.batch_size, action_num], name=\"next_Y_ph\")\n",
      "    reward_ph = tf.placeholder(tf.float32, [config.batch_size], name=\"reward_ph\")\n",
      "    \n",
      "    ph_lst = [input_state_ph, action_ph, Y_ph, next_Y_ph, reward_ph]\n",
      "    \n",
      "    q = tf.FIFOQueue(2, [ph.dtype for ph in ph_lst],\n",
      "                     [ph.get_shape() for ph in ph_lst])\n",
      "    enqueue_op = q.enqueue(ph_lst)\n",
      "    input_state, action, Y, next_Y, reward = q.dequeue()\n",
      "    \n",
      "    # so that i can feed inputs with different batch sizes. \n",
      "    input_state = tf.placeholder_with_default(input_state, shape=tf.TensorShape([None]).concatenate(input_state.get_shape()[1:]))\n",
      "    action = tf.placeholder_with_default(action, shape=[None])\n",
      "    next_input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"next_input_state_placeholder\")\n",
      "    \n",
      "    with tf.variable_scope(\"DQN\"):\n",
      "        Q, R, predicted_next_Q = createQNetwork(input_state, action, config, \"DQN\")\n",
      "        DQN_params = tf.get_collection(\"DQN_weights\")\n",
      "        max_action_DQN = tf.argmax(Q, 1)\n",
      "    with tf.variable_scope(\"DQNTarget\"):\n",
      "        # pasing an action is useless because the target never runs the next_Y_prediction but it is needed for the code to work\n",
      "        QT, RT, predicted_next_QT = createQNetwork(next_input_state_ph, action, config, \"DQNT\")\n",
      "        DQNT_params = tf.get_collection(\"DQNT_weights\")\n",
      "\n",
      "    # DQN summary\n",
      "    for i in range(action_num):\n",
      "        dqni = tf.scalar_summary(\"DQN/action\"+str(i), Q[0, i])\n",
      "        tf.add_to_collection(\"DQN_summaries\", dqni)\n",
      "\n",
      "    sync_DQNT_op = [DQNT_params[i].assign(DQN_params[i]) for i in range(len(DQN_params))]\n",
      "    \n",
      "    train_op = build_train_op(Q, Y, R, reward, predicted_next_Q, next_Y, action, config)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_action_ = action\n",
      "_R_ = R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enqueue_from_RM():\n",
      "    while True:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM.sample_transition_batch()\n",
      "        if global_step % config.save_summary_rate == 0:\n",
      "            QT_np, DQNT_summary_str = sess.run([QT, DQNT_summary_op], feed_dict={next_input_state_ph:next_state_batch})\n",
      "            summary_writter.add_summary(DQNT_summary_str, global_step)\n",
      "        else:\n",
      "            QT_np = sess.run(QT, feed_dict={next_input_state_ph:next_state_batch})\n",
      "            \n",
      "        DQNT_max_action_batch = np.max(QT_np, 1)\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + config.gamma * DQNT_max_action_batch[i])\n",
      "        feed_dict={input_state_ph:state_batch, action_ph:action_batch, next_input_state_ph:next_state_batch, Y_ph:Y, next_Y_ph:QT_np, reward_ph:reward_batch}\n",
      "        sess.run(enqueue_op, feed_dict=feed_dict)\n",
      "        \n",
      "enqueue_from_RM_thread = threading.Thread(target=enqueue_from_RM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_params():\n",
      "    if global_step > config.steps_before_training:\n",
      "        if enqueue_from_RM_thread.isAlive() == False:\n",
      "            flush_print(\"starting enqueue thread\")\n",
      "            enqueue_from_RM_thread.start()\n",
      "            \n",
      "        if global_step % config.save_summary_rate == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op])\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "        else:\n",
      "             _ = sess.run(train_op)            \n",
      "\n",
      "        if global_step % config.sync_rate == 0:\n",
      "            sess.run(sync_DQNT_op)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess_config = tf.ConfigProto()\n",
      "sess_config.allow_soft_placement = True\n",
      "sess_config.gpu_options.allow_growth = True\n",
      "sess_config.log_device_placement = True\n",
      "sess = tf.Session(config=sess_config)\n",
      "saver = tf.train.Saver(max_to_keep = 20)\n",
      "sess.run(tf.initialize_variables(DQN_params))\n",
      "sess.run(tf.initialize_variables(DQNT_params))\n",
      "sess.run(tf.initialize_all_variables())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#graph_vars = [(v.op.name) for v in tf.get_collection(tf.GraphKeys.VARIABLES)]\n",
      "#checkpoint_vars = [unicode(v) for v in zip(*tf.contrib.framework.list_variables(\"checkpoint/71.ckpt-8000\"))[0]]\n",
      "#common_var_names = [v for v in graph_vars if v in checkpoint_vars]\n",
      "#common_vars = [v for v in tf.all_variables() if v.op.name in common_var_names]\n",
      "#common_saver = tf.train.Saver(common_vars)\n",
      "#common_saver.restore(sess, \"checkpoint/71.ckpt-8000\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#geneate a new set of paths\n",
      "run_list = os.listdir(\"log\")\n",
      "int_run_list = [int(r) for r in run_list] + [0]\n",
      "run_name = str(max(int_run_list) + 1)\n",
      "#run_name = str(3)\n",
      "checkpoint_path = \"checkpoint/\" + run_name + \".ckpt\"\n",
      "log_path = \"log/\"+ run_name\n",
      "print(run_name)\n",
      "DQN_summary_op = tf.merge_summary(tf.get_collection(\"DQN_summaries\") + \\\n",
      "                                  tf.get_collection(\"DQN_prediction_summaries\"))\n",
      "DQNT_summary_op = tf.merge_summary(tf.get_collection(\"DQNT_summaries\"))\n",
      "summary_writter = tf.train.SummaryWriter(log_path, sess.graph, flush_secs=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#image_summary = tf.image_summary(\"input image\", tf.transpose(tf.gather(input_state, [0]), [3,1,2,0]), max_images=4, name=\"input_state\")\n",
      "#DQN_summary_op = tf.merge_summary(tf.get_collection(\"DQN_summaries\") + \\\n",
      "#                                  tf.get_collection(\"DQN_summaries_prediction\") + \\\n",
      "#                                  [image_summary] + \\\n",
      "#                                  [tf.scalar_summary(\"queue_size\", q.size())])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def e_greedy_action(epsilon, state):\n",
      "        if np.random.uniform() < epsilon:\n",
      "            action = random.randint(0, action_num - 1)\n",
      "        else:\n",
      "            action = np.argmax(sess.run(Q, feed_dict={input_state:state})[0])\n",
      "        return action\n",
      "    \n",
      "def e_greedy_planning_action(epsilon, state):\n",
      "        if np.random.uniform() < epsilon:\n",
      "            a = random.randint(0, action_num - 1)\n",
      "        else:\n",
      "            next_Q = []\n",
      "            predicned_Rs, next_Q_0 = sess.run([_R_, predicted_next_Q], feed_dict={input_state:state, _action_:[0]})\n",
      "            a = 1 \n",
      "            next_Q.append(np.max(next_Q_0))\n",
      "            next_Q.append(np.max(sess.run(predicted_next_Q,feed_dict={input_state:state, _action_:[1]})))\n",
      "            next_Q.append(np.max(sess.run(predicted_next_Q,feed_dict={input_state:state, _action_:[2]})))\n",
      "            next_Q.append(np.max(sess.run(predicted_next_Q,feed_dict={input_state:state, _action_:[3]})))\n",
      "            predicted_Q = []\n",
      "            for i in range(4):\n",
      "                predicted_Q.append(predicned_Rs[0][i] + config.gamma* next_Q[i])\n",
      "            a = np.argmax(predicted_Q)\n",
      "        return a\n",
      "\n",
      "def greedy_run(epsilon, n, use_planning=False):\n",
      "    ale.reset_game()\n",
      "    R_list = []\n",
      "    for episode in range(n):\n",
      "        state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        R = 0\n",
      "        while ale.game_over() == False:\n",
      "            if use_planning:\n",
      "                action = e_greedy_planning_action(epsilon, state)\n",
      "            else:\n",
      "                action = e_greedy_action(epsilon, state)\n",
      "            reward = ale.act(action_map[action])\n",
      "            state = preprocess(ale.getScreenGrayscale(), state)\n",
      "            R += reward\n",
      "        R_list.append(R)\n",
      "        ale.reset_game()\n",
      "    return R_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#saver.restore(sess, \"checkpoint/82.ckpt-4000\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_step = 0\n",
      "global_episode = 0\n",
      "logging = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "num_episodes = 10000\n",
      "initial_episode = global_episode\n",
      "for episode in range(global_episode, num_episodes + global_episode):\n",
      "    global state\n",
      "    state = np.zeros((1, 84, 84, config.buff_size), dtype=np.uint8)\n",
      "    state = preprocess(ale.getScreenGrayscale(), state)\n",
      "    R = 0\n",
      "    ep_begin_t = time.time()\n",
      "    isTerminal = False\n",
      "    episode_begining_step = global_step\n",
      "    while isTerminal == False:\n",
      "        action = e_greedy_action(get_epsilon(), state)\n",
      "        reward = ale.act(action_map[action])\n",
      "        clipped_reward = max(-1, min(1, reward))\n",
      "        R += reward\n",
      "        if ale.game_over():\n",
      "            isTerminal = True\n",
      "        RM.add(state[0, :, :, config.buff_size -1], action, clipped_reward, isTerminal)\n",
      "        update_params()\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        global_step += 1\n",
      "    ep_duration = time.time() - ep_begin_t\n",
      "    if logging and episode%50 == 0 and episode != 0 or num_episodes == episode:\n",
      "        episode_online_summary = tf.Summary(value=[tf.Summary.Value(tag=\"online/epsilon\", simple_value=get_epsilon()), \n",
      "                                    tf.Summary.Value(tag=\"online/R\", simple_value=R),\n",
      "                                    tf.Summary.Value(tag=\"online/steps_in_episode\", simple_value= global_step - episode_begining_step),     \n",
      "                                    tf.Summary.Value(tag=\"online/global_step\", simple_value = global_step),           \n",
      "                                    tf.Summary.Value(tag=\"online/ep_duration_seconds\", simple_value=ep_duration)])\n",
      "        summary_writter.add_summary(episode_online_summary, global_episode)\n",
      "    # log percent\n",
      "    if logging and logging==True and episode%500 == 0 and episode != 0 or num_episodes == episode:\n",
      "        percent = int(float(episode - initial_episode)/num_episodes * 100)\n",
      "        print(\"%i%% -- epsilon:%.2f\"%(percent, get_epsilon()))\n",
      "    # save\n",
      "    if logging and episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        print(\"saving checkpoint at episode \" + str(episode))\n",
      "        saver.save(sess, checkpoint_path, episode)\n",
      "        \n",
      "    # performance summary\n",
      "    if logging and episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        R_list = greedy_run(epsilon = 0.05, n=20)\n",
      "        Planning_R_list = greedy_run(epsilon = 0.05, n=20, use_planning=True)\n",
      "        performance_summary = tf.Summary(value=[tf.Summary.Value(tag=\"R/average\", simple_value=sum(R_list)/len(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/max\", simple_value=max(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/min\", simple_value=min(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/average_planning\", simple_value=sum(Planning_R_list)/len(Planning_R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/max_planning\", simple_value=max(Planning_R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/min_planning\", simple_value=min(Planning_R_list)),\n",
      "                                      ])\n",
      "        summary_writter.add_summary(performance_summary, global_step)\n",
      "        \n",
      "    global_episode += 1\n",
      "    ale.reset_game()\n",
      "print(\"==\")\n",
      "print((time.time() - t)/60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "starting enqueue thread\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tensorflow.python.client import timeline\n",
      "run_metadata = tf.RunMetadata()\n",
      "if enqueue_from_RM_thread.isAlive():\n",
      "    sess.run(train_op, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\n",
      "else: print \"cannot run if enqueue thread is not alive\"\n",
      "trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
      "trace_file = open(\"timeline.ctf.json\", \"w\")\n",
      "trace_file.write(trace.generate_chrome_trace_format(True, True))\n",
      "trace_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cannot run if enqueue thread is not alive\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " saver.save(sess, checkpoint_path, episode)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "'checkpoint/91.ckpt-5'"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RM_path = \"checkpoint/RM_\"+run_name+\"_\"+str(global_step)\n",
      "def RM_save(self):\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        np.save(RM_path+name, array)\n",
      "    \n",
      "def RM_load():\n",
      "    self = ReplayMemory(config)\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    self.filled = True\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        array[:] = np.load(RM_path+name+\".npy\")\n",
      "    return self\n",
      "\n",
      "def RM_info(RM):\n",
      "    print(\"current :\"+str(RM.current))\n",
      "    print(\"step :\"+str(RM.step))\n",
      "    print(\"capacity :\"+str(RM.capacity))\n",
      "    print(\"filled :\"+str(RM.filled))\n",
      "    print(\"batch_size :\"+str(RM.batch_size))\n",
      "    \n",
      "print(RM_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/RM_29_1640904\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.save(sess, checkpoint_path, episode)\n",
      "print(checkpoint_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/62.ckpt\n"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.restore(sess, checkpoint_path+\"-\"+str(episode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 468
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "color = \"Greys_r\"\n",
      "def show_state(state):\n",
      "    fig = plt.figure()\n",
      "    for i in range(0, 4):\n",
      "        a=fig.add_subplot(1,4,i+1)\n",
      "        plt.axis(\"off\")\n",
      "        plt.title(str(i))\n",
      "        plt.imshow(state[:,:,i], color)\n",
      "\n",
      "def show_frame(frame):\n",
      "    fig = plt.figure()\n",
      "    plt.axis(\"off\")\n",
      "    if len(frame.shape) == 2:\n",
      "        plt.imshow(frame, color)\n",
      "    elif len(frame.shape) == 3:\n",
      "        plt.imshow(frame[:,:,config.buff_size -1], color)\n",
      "    elif len(frame.shape) == 4:\n",
      "        plt.imshow(frame[0, :,:,config.buff_size -1], color)\n",
      "    else:\n",
      "        print(\"Wrong shape\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}